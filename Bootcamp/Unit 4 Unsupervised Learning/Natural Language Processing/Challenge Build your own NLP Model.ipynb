{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import gensim\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import xgboost\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hamlet Raw:\n",
      " [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a\n",
      "\n",
      "--------------\n",
      "Macbeth Raw:\n",
      " [The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lig\n"
     ]
    }
   ],
   "source": [
    "# Print the first 100 characters of Hamlet\n",
    "print('\\nHamlet Raw:\\n', hamlet[0:100])\n",
    "# Print the first 100 characters of Macbeth\n",
    "print('\\n--------------\\nMacbeth Raw:\\n', macbeth[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamlet:\n",
      " [The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo a\n",
      "\n",
      "Macbeth:\n",
      " [The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lig\n"
     ]
    }
   ],
   "source": [
    "# Print the first 100 characters of Alice again.\n",
    "print('Hamlet:\\n', hamlet[0:100])\n",
    "# All done with cleanup? Let's see how it looks.\n",
    "print('\\nMacbeth:\\n', macbeth[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "hamlet = text_cleaner(hamlet)\n",
    "macbeth = text_cleaner(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=False):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_freq = word_frequencies(hamlet_doc).most_common(2000)\n",
    "macbeth_freq = word_frequencies(macbeth_doc).most_common(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "hamlet_common = [pair[0] for pair in hamlet_freq]\n",
    "macbeth_common = [pair[0] for pair in macbeth_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "hamlet_unique = set(hamlet_common) - set(macbeth_common)\n",
    "macbeth_unique = set(macbeth_common) - set(hamlet_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "hamlet_sents = [[sent, \"Hamlet\"] for sent in hamlet_doc.sents]\n",
    "macbeth_sents = [[sent, \"Macbeth\"] for sent in macbeth_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(hamlet_sents + macbeth_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Label\n",
    "sentences[\"Hamlet?\"] = np.where(sentences[1]== \"Hamlet\", 1, 0)\n",
    "\n",
    "# Convert sentences tuples to string\n",
    "sentences[0] = sentences[0].astype(str)\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences[0] = sentences.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "\n",
    "# Remove all stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "sentences[0] = sentences[0].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# Lower Case everything\n",
    "sentences[0] = sentences[0].astype(str)\n",
    "sentences[0] = sentences[0].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all punctuations\n",
    "from string import punctuation\n",
    "sentences[0] = sentences[0].apply(lambda x: ''.join(c for c in x if c not in punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Hamlet?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actus primus</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scoena prima</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enter barnardo francisco two centinels</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barnardo</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who s</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0       1  Hamlet?\n",
       "0                            actus primus   Hamlet        1\n",
       "1                            scoena prima   Hamlet        1\n",
       "2  enter barnardo francisco two centinels   Hamlet        1\n",
       "3                                barnardo   Hamlet        1\n",
       "4                                   who s   Hamlet        1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sentences[0]\n",
    "y = sentences['Hamlet?']\n",
    "\n",
    "# Split data into training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(x_train)\n",
    "\n",
    "xtrain_count = count_vect.transform(x_train)\n",
    "xtest_count = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(sentences[0])\n",
    "xtrain_tfidf =  tfidf_vect.transform(x_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(x_test)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(sentences[0])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7595959595959596\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), y_train, xtest_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.7474747474747475\n",
      "Xgb, CharLevel Vectors:  0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), y_train, xtest_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram.tocsc(), y_train, xtest_tfidf_ngram.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803030303030303\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=10)\n",
    "train = rfc.fit(xtrain_tfidf, y_train)\n",
    "\n",
    "print(rfc.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3956, 5000) (3956,)\n",
      "Training set score: 0.8968655207280081\n",
      "\n",
      "Test set score: 0.798989898989899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(xtrain_tfidf, y_train)\n",
    "print(xtrain_tfidf.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(xtrain_tfidf, y_train))\n",
    "print('\\nTest set score:', lr.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8882709807886754\n",
      "\n",
      "Test set score: 0.794949494949495\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier(n_estimators=500)\n",
    "train = clf.fit(xtrain_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(xtrain_tfidf, y_train))\n",
    "print('\\nTest set score:', clf.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7606060606060606\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=8)\n",
    "train = rfc.fit(xtrain_count, y_train)\n",
    "\n",
    "print(rfc.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3956, 5726) (3956,)\n",
      "Training set score: 0.9436299292214358\n",
      "\n",
      "Test set score: 0.8080808080808081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(xtrain_count, y_train)\n",
    "print(xtrain_count.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(xtrain_count, y_train))\n",
    "print('\\nTest set score:', lr.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7591001011122346\n",
      "\n",
      "Test set score: 0.7676767676767676\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(xtrain_count, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(xtrain_count, y_train))\n",
    "print('\\nTest set score:', clf.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words that are found in both texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Text</th>\n",
       "      <th>Hamlet?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quintessence</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>courteous</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>purples</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comingled</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leasure</td>\n",
       "      <td>Hamlet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Words    Text  Hamlet?\n",
       "0  quintessence  Hamlet        1\n",
       "1     courteous  Hamlet        1\n",
       "2       purples  Hamlet        1\n",
       "3     comingled  Hamlet        1\n",
       "4       leasure  Hamlet        1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = sentences[sentences[1]=='Hamlet']\n",
    "hamlet = hamlet[0]\n",
    "\n",
    "macbeth = sentences[sentences[1]=='Macbeth']\n",
    "macbeth = macbeth[0]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_hamlet = [word_tokenize(i) for i in list(hamlet)]\n",
    "tokenized_macbeth = [word_tokenize(i) for i in list(macbeth)]\n",
    "\n",
    "from itertools import chain\n",
    "tokenized_hamlet = list(chain.from_iterable(tokenized_hamlet))\n",
    "tokenized_macbeth = list(chain.from_iterable(tokenized_macbeth))\n",
    "\n",
    "hamlet_unique = set(tokenized_hamlet) - set(tokenized_macbeth)\n",
    "macbeth_unique = set(tokenized_macbeth) - set(tokenized_hamlet)\n",
    "\n",
    "features1 = pd.DataFrame()\n",
    "features1['Words'] = list(hamlet_unique)\n",
    "features1['Text'] = 'Hamlet'\n",
    "\n",
    "features2 = pd.DataFrame()\n",
    "features2['Words'] = list(macbeth_unique)\n",
    "features2['Text'] = 'Macbeth'\n",
    "\n",
    "features = features1.append(features2)\n",
    "\n",
    "features[\"Hamlet?\"] = np.where(features['Text'] == 'Hamlet', 1, 0)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features['Words']\n",
    "y = features['Hamlet?']\n",
    "\n",
    "# Split data into training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectorizer object\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(x_train)\n",
    "\n",
    "xtrain_count = count_vect.transform(x_train)\n",
    "xtest_count = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(sentences[0])\n",
    "xtrain_tfidf =  tfidf_vect.transform(x_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6533742331288344\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(xtrain_tfidf, y_train)\n",
    "\n",
    "print(rfc.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 5000) (3911,)\n",
      "Training set score: 0.6205574021989261\n",
      "\n",
      "Test set score: 0.6533742331288344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(xtrain_tfidf, y_train)\n",
    "print(xtrain_tfidf.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(xtrain_tfidf, y_train))\n",
    "print('\\nTest set score:', lr.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6205574021989261\n",
      "\n",
      "Test set score: 0.6533742331288344\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(xtrain_tfidf, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(xtrain_tfidf, y_train))\n",
    "print('\\nTest set score:', clf.score(xtest_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6533742331288344\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(xtrain_count, y_train)\n",
    "\n",
    "print(rfc.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3911, 3911) (3911,)\n",
      "Training set score: 0.6205574021989261\n",
      "\n",
      "Test set score: 0.6533742331288344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(xtrain_count, y_train)\n",
    "print(xtrain_count.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(xtrain_count, y_train))\n",
    "print('\\nTest set score:', lr.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6205574021989261\n",
      "\n",
      "Test set score: 0.6533742331288344\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(xtrain_count, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(xtrain_count, y_train))\n",
    "print('\\nTest set score:', clf.score(xtest_count, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
