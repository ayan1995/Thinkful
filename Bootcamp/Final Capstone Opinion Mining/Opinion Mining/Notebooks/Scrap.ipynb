{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Spacy parser\n",
    "# spacy.load('en')\n",
    "# parser = English()\n",
    "\n",
    "# # Function to tokenize text\n",
    "# def tokenize(text):\n",
    "#     lda_tokens = []\n",
    "#     tokens  = parser(text)\n",
    "#     for token in tokens:\n",
    "#         if token.orth_.isspace():\n",
    "#             continue\n",
    "#         elif token.like_url:\n",
    "#             lda_tokens.append('URL')\n",
    "#         elif token.orth_.startswith('@'):\n",
    "#             lda_tokens.append('SCREEN_NAME')\n",
    "#         else:\n",
    "#             lda_tokens.append(token.lower_)\n",
    "#     return lda_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mallet LDA\n",
    "\n",
    "# mallet_path = '/Users/ayankarim/mallet-2.0.8/bin/mallet'\n",
    "\n",
    "# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, id2word=id2word, iterations=1000, random_seed=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show topics\n",
    "# pprint(ldamallet.show_topics(num_words=15, formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert documents to sentences\n",
    "# def sent_to_words(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "        \n",
    "# data = list(df['text'])\n",
    "# data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Bigrams and Trigrams\n",
    "\n",
    "# # Build the models\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "# # fast way to get a sentece clubbed as a bigram/trigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Stopwords, make bigrams and lemmatize\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['pixel', 'iphone', 'samsung', 'apple', 'essential', 'xs', 'max', \n",
    "#                   'huawei', 'galaxy', 'note', 'moto', 'oneplus', 'android', 'mate', 'pro', 'lg', 'sony', 'razer', 'phone', 'company', \n",
    "#                   'smartphone', 'google', 'thinq', 'nokia', 'htc', 'xperia', 'xz'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Stop Words\n",
    "# data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gather tfidf scores\n",
    "# tfidf = models.TfidfModel(corpus, id2word = id2word)\n",
    "\n",
    "# # filter low value words\n",
    "# low_value = 0.025\n",
    "\n",
    "# for i in range(0, len(corpus)):\n",
    "#     bow = corpus[i]\n",
    "#     low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "#     low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "#     new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "\n",
    "#     #reassign        \n",
    "#     corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns the topics to the documents in corpus\n",
    "# lda_corpus = lda[corpus]\n",
    "\n",
    "# # Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# # To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "# scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "#                       for topic in [doc for doc in lda_corpus]]))\n",
    "# threshold = sum(scores)/len(scores)\n",
    "# print (threshold)\n",
    "\n",
    "# reliability = [j for i,j in zip(lda_corpus,data) if i[0][1] > threshold]\n",
    "# functionality = [j for i,j in zip(lda_corpus,data) if i[1][1] > threshold]\n",
    "# design = [j for i,j in zip(lda_corpus,data) if i[2][1] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelled_df = df\n",
    "\n",
    "# # Assign labels\n",
    "# labelled_df['reliability'] = \"\"\n",
    "# labelled_df['functionality'] = \"\"\n",
    "# labelled_df['design'] = \"\"\n",
    "\n",
    "# labelled_df['reliability'] = np.where(labelled_df['text'].isin(reliability), 'reliability', None)\n",
    "# labelled_df['functionality'] = np.where(labelled_df['text'].isin(functionality), 'functionality', None)\n",
    "# labelled_df['design'] = np.where(labelled_df['text'].isin(design), 'design', None)\n",
    "\n",
    "# labelled_df['labelled'] = labelled_df[['reliability', 'functionality', 'design']].values.tolist()\n",
    "# labelled_df['labelled'] = labelled_df['labelled'].apply(lambda x: list(filter(lambda a: a != None, x)))\n",
    "# labelled_df = labelled_df.drop(['reliability', 'functionality', 'design'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate SVM Multi-Label Classifier\n",
    "\n",
    "# y = mlb.fit_transform(labelled_df.labelled)\n",
    "# X = labelled_df.text\n",
    "\n",
    "# text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('clf-svm', LabelPowerset(\n",
    "#                              SGDClassifier(loss='hinge', penalty='l2',\n",
    "#                                            alpha=1e-3, max_iter=6, random_state=42)))])\n",
    "# _ = text_clf_svm.fit(X, y)\n",
    "# predicted_svm = text_clf_svm.predict(X)\n",
    "\n",
    "# #Calculate accuracy\n",
    "# np.mean(predicted_svm == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # View accuracy scores on classifying each author (precission, recall, f1-score and support)\n",
    "# print(metrics.classification_report(y, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPhoneX_pos = df_iphonex['pos'].sum()\n",
    "# S9_pos = df_s9['pos'].sum()\n",
    "# IPhoneX_neg = df_iphonex['neg'].sum()\n",
    "# S9_neg = df_s9['neg'].sum()\n",
    "\n",
    "# compare_bothphones = pd.DataFrame()\n",
    "# compare_bothphones['IPhoneX_pos'] = [IPhoneX_pos]\n",
    "# compare_bothphones['S9_pos'] = [S9_pos]\n",
    "# compare_bothphones['pos/pos'] = [(IPhoneX_pos/S9_pos)]\n",
    "\n",
    "# compare_bothphones['IPhoneX_neg'] = [IPhoneX_neg]\n",
    "# compare_bothphones['S9_neg'] = [S9_neg]\n",
    "# compare_bothphones['pos/neg'] = [(IPhoneX_neg/S9_neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
