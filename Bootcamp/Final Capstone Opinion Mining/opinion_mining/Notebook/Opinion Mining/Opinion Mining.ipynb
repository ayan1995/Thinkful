{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies and modules\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each json file\n",
    "with open('iphonex_digtrends.json') as f:\n",
    "    iphonex_digtrends = json.load(f)\n",
    "\n",
    "with open('iphonex_gizmodo.json') as f:\n",
    "    iphonex_gizmodo = json.load(f)\n",
    "\n",
    "with open('iphonex_techradar.json') as f:\n",
    "    iphonex_techradar = json.load(f)\n",
    "\n",
    "with open('S9_digtrends.json') as f:\n",
    "    S9_digtrends = json.load(f)\n",
    "\n",
    "with open('S9_gizmodo.json') as f:\n",
    "    S9_gizmodo = json.load(f)\n",
    "\n",
    "with open('S9_techradar.json') as f:\n",
    "    S9_techradar = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "\n",
    "    text = str(text).replace(\"\\n\", \"\")\n",
    "    text = str(text).replace(\"\\t\", \"\")\n",
    "    text = str(text).replace(\"\\\\n\", \"\")\n",
    "    text = str(text).replace(\"\\\\t\", \"\")\n",
    "    text = str(text).replace(\"\\\\\", \"\")\n",
    "    text = str(text).replace(\"xa0\", \" \")\n",
    "    text = str(text).replace(\"\\'\", \"\")\n",
    "    text = re.sub(\"<p>\", \"\", str(text))\n",
    "    text = re.sub(\"</p>\", \"\", str(text))\n",
    "    text = re.sub(\"</a>\", \"\", str(text))\n",
    "    text = re.sub('<[^>]+>', \"\", str(text))\n",
    "    text = str(text).replace(\"\\\\u2019\", \"\")\n",
    "    text = str(text).replace(\"\\\\u2013\", \"\")\n",
    "    text = str(text).replace(\"\\\\u2018\", \"\")\n",
    "    text = str(text).replace(\"\\\\u00a0\", \"\")\n",
    "    text = str(text).replace(\"\\\\u00a3\", \"\")\n",
    "    text = str(text).replace(\"\\u2014\", \"\")\n",
    "    text = str(text).replace(\"\\u201d\", \"\")\n",
    "    text = str(text).replace(\"\\u201c\", \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate each JSON file into a data frame\n",
    "\n",
    "iphonex_digtrends = pd.DataFrame.from_dict(iphonex_digtrends, orient='columns')\n",
    "iphonex_gizmodo = pd.DataFrame.from_dict(iphonex_gizmodo, orient='columns')\n",
    "iphonex_techradar = pd.DataFrame.from_dict(iphonex_techradar, orient='columns')\n",
    "S9_digtrends = pd.DataFrame.from_dict(S9_digtrends, orient='columns')\n",
    "S9_gizmodo = pd.DataFrame.from_dict(S9_gizmodo, orient='columns')\n",
    "S9_techradar = pd.DataFrame.from_dict(S9_techradar, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean text\n",
    "def clean_text(df):\n",
    "    # Convert lists to strings and remove brackets\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['author'] = df['author'].astype(str)\n",
    "\n",
    "    df['text'] = df['text'].map(lambda x: x.strip('[]'))\n",
    "    df['author'] = df['author'].map(lambda x: x.strip('[]'))\n",
    "\n",
    "    # Clean text\n",
    "    df['text'] = df['text'].apply(lambda x: text_cleaner(x))\n",
    "    df['title'] = df['title'].apply(lambda x: text_cleaner(x))\n",
    "    df['author'] = df['author'].apply(lambda x: text_cleaner(x))\n",
    "\n",
    "    \n",
    "# Put dataframes into a list to iterate through\n",
    "dataframes = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar, S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "\n",
    "# Clean each Data Frame\n",
    "for dataframe in dataframes:\n",
    "    clean_text(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label all the rows in the dataframe for the phone that the article is talking about\n",
    "\n",
    "iphones = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar]\n",
    "s9s = [S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "\n",
    "for dataframe in iphones:\n",
    "    dataframe['phone'] = 'IPhone X'\n",
    "    \n",
    "for dataframe in s9s:\n",
    "    dataframe['phone'] = 'Samsung Galaxy S9'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the dataframes into one dataframe\n",
    "all_frames = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar, S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "df = pd.concat(all_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric Brackett</td>\n",
       "      <td>The iPhone X launched to stellar reviews and e...</td>\n",
       "      <td>Shrinking demand forces Apple to slow down iPh...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lucas Coll</td>\n",
       "      <td>When it comes to high-quality devices, like th...</td>\n",
       "      <td>Looking to upgrade? These are the best iPhone ...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simon Hill</td>\n",
       "      <td>The iPhone X is completely different from any ...</td>\n",
       "      <td>The most common iPhone X problems, and how to ...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trevor Mogg</td>\n",
       "      <td>If you’re in the market for an iPhone X, and p...</td>\n",
       "      <td>This $4,600 solar charger comes with an iPhone...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark Jansen</td>\n",
       "      <td>, The initial estimates, set during the Novemb...</td>\n",
       "      <td>Apple will halve iPhone X production after lim...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               text  \\\n",
       "0  Eric Brackett  The iPhone X launched to stellar reviews and e...   \n",
       "1     Lucas Coll  When it comes to high-quality devices, like th...   \n",
       "2     Simon Hill  The iPhone X is completely different from any ...   \n",
       "3    Trevor Mogg  If you’re in the market for an iPhone X, and p...   \n",
       "4    Mark Jansen  , The initial estimates, set during the Novemb...   \n",
       "\n",
       "                                               title     phone  \n",
       "0  Shrinking demand forces Apple to slow down iPh...  IPhone X  \n",
       "1  Looking to upgrade? These are the best iPhone ...  IPhone X  \n",
       "2  The most common iPhone X problems, and how to ...  IPhone X  \n",
       "3  This $4,600 solar charger comes with an iPhone...  IPhone X  \n",
       "4  Apple will halve iPhone X production after lim...  IPhone X  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Processing Options for texts\n",
    "\n",
    "# # Tokenize text\n",
    "# df['text'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "# df['title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "# # Remove Stopwords, or keep it, might be important for aspect based semantics\n",
    "# stop = stopwords.words('english')\n",
    "# df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# df['title'] = df['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# # Lowercase everything\n",
    "# df['text'] = df['text'].astype(str)\n",
    "# df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# df['title'] = df['title'].astype(str)\n",
    "# df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "\n",
    "# # remove all punctuations\n",
    "# df['text'] = df['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "# df['title'] = df['title'].apply(lambda x: ''.join(c for c in x if c not in punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download wordnet to find meaning of words, synonyms and antonyms\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "\n",
    "# # Function to lemmatize and more words to their root\n",
    "# def get_lemma(word):\n",
    "#     lemma = wn.morphy(word)\n",
    "#     if lemma is None:\n",
    "#         return word\n",
    "#     else:\n",
    "#         return lemma\n",
    "    \n",
    "# # Compile set of stopwords\n",
    "# nltk.download('stopwords')\n",
    "# en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Process Text Column\n",
    "\n",
    "# text_data = []\n",
    "\n",
    "# # Prepare training set for LDA\n",
    "# tokens = df['text'].apply(lambda x: prepare_text_for_lda(x))\n",
    "\n",
    "# # Prepare Dataframe for later\n",
    "# df['text'] = df['text'].apply(lambda x: prepare_text_for_lda(x))\n",
    "\n",
    "# # Append tokenized text to list of tokenized data\n",
    "# null = tokens.apply(lambda x: text_data.append(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Process Title Column\n",
    "\n",
    "# # Tokenize text\n",
    "# df['title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "# # Remove Stopwords, or keep it, might be important for aspect based semantics\n",
    "# stop = stopwords.words('english')\n",
    "# df['title'] = df['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# # Lowercase everything\n",
    "# df['title'] = df['title'].astype(str)\n",
    "# df['title'] = df['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "spacy.load('en')\n",
    "parser = English()\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens  = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eric Brackett</td>\n",
       "      <td>The iPhone X launched to stellar reviews and e...</td>\n",
       "      <td>Shrinking demand forces Apple to slow down iPh...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lucas Coll</td>\n",
       "      <td>When it comes to high-quality devices, like th...</td>\n",
       "      <td>Looking to upgrade? These are the best iPhone ...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simon Hill</td>\n",
       "      <td>The iPhone X is completely different from any ...</td>\n",
       "      <td>The most common iPhone X problems, and how to ...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trevor Mogg</td>\n",
       "      <td>If you’re in the market for an iPhone X, and p...</td>\n",
       "      <td>This $4,600 solar charger comes with an iPhone...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mark Jansen</td>\n",
       "      <td>, The initial estimates, set during the Novemb...</td>\n",
       "      <td>Apple will halve iPhone X production after lim...</td>\n",
       "      <td>IPhone X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                               text  \\\n",
       "0  Eric Brackett  The iPhone X launched to stellar reviews and e...   \n",
       "1     Lucas Coll  When it comes to high-quality devices, like th...   \n",
       "2     Simon Hill  The iPhone X is completely different from any ...   \n",
       "3    Trevor Mogg  If you’re in the market for an iPhone X, and p...   \n",
       "4    Mark Jansen  , The initial estimates, set during the Novemb...   \n",
       "\n",
       "                                               title     phone  \n",
       "0  Shrinking demand forces Apple to slow down iPh...  IPhone X  \n",
       "1  Looking to upgrade? These are the best iPhone ...  IPhone X  \n",
       "2  The most common iPhone X problems, and how to ...  IPhone X  \n",
       "3  This $4,600 solar charger comes with an iPhone...  IPhone X  \n",
       "4  Apple will halve iPhone X production after lim...  IPhone X  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Opinion Lexicon\n",
    "\n",
    "negatives = open('negative-words.txt', encoding = 'latin-1')\n",
    "positives = open('positive-words.txt', encoding = 'latin-1')\n",
    "\n",
    "# Read file\n",
    "neg_unedit = [line.strip() for line in negatives.readlines()]\n",
    "pos_unedit = [line.strip() for line in positives.readlines()]\n",
    "\n",
    "# Extract only the list of words in the lexicon\n",
    "neg = neg_unedit[31:]\n",
    "pos = pos_unedit[30:]\n",
    "\n",
    "# Compile opinion words\n",
    "opinion_words = neg + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "from spacy import displacy\n",
    "import gensim\n",
    "\n",
    "# Load Neural Coreference to replace parse text and replace pronouns\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature_sentiment function\n",
    "def feature_sentiment(sentence):\n",
    "    '''\n",
    "    input: dictionary and sentence\n",
    "    function: appends dictionary with new features if the feature did not exist previously,\n",
    "              then updates sentiment to each of the new or existing features\n",
    "    output: updated dictionary\n",
    "    '''\n",
    "    \n",
    "    sent_dict = Counter()\n",
    "    sentence = nlp(sentence)\n",
    "    debug = 0\n",
    "    for token in sentence:\n",
    "    #    print(token.text,token.dep_, token.head, token.head.dep_)\n",
    "        # check if the word is an opinion word, then assign sentiment\n",
    "        if token.text in opinion_words:\n",
    "            sentiment = 1 if token.text in pos else -1\n",
    "            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n",
    "            # but happens to be an opinion word, ignore and pass\n",
    "            if (token.dep_ == \"advmod\"):\n",
    "                continue\n",
    "            elif (token.dep_ == \"amod\"):\n",
    "                sent_dict[token.head.text] += sentiment\n",
    "            # for opinion words that are adjectives, adverbs, verbs...\n",
    "            else:\n",
    "                for child in token.children:\n",
    "                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n",
    "                    # This could be better updated for modifiers that either positively or negatively emphasize\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if child.dep_ == \"neg\":\n",
    "                        sentiment *= -1\n",
    "                for child in token.children:\n",
    "                    # if verb, check if there's a direct object\n",
    "                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n",
    "                        sent_dict[child.text] += sentiment\n",
    "                        # check for conjugates (a AND b), then add both to dictionary\n",
    "                        subchildren = []\n",
    "                        conj = 0\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.text == \"and\":\n",
    "                                conj=1\n",
    "                            if (conj == 1) and (subchild.text != \"and\"):\n",
    "                                subchildren.append(subchild.text)\n",
    "                                conj = 0\n",
    "                        for subchild in subchildren:\n",
    "                            sent_dict[subchild] += sentiment\n",
    "\n",
    "                # check for negation\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n",
    "                        sentiment *= 1.5\n",
    "                    # check for negation words and flip the sign of sentiment\n",
    "                    if (child.dep_ == \"neg\"): \n",
    "                        sentiment *= -1\n",
    "                \n",
    "                # check for nouns\n",
    "                for child in token.head.children:\n",
    "                    noun = \"\"\n",
    "                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n",
    "                        noun = child.text\n",
    "                        # Check for compound nouns\n",
    "                        for subchild in child.children:\n",
    "                            if subchild.dep_ == \"compound\":\n",
    "                                noun = subchild.text + \" \" + noun\n",
    "                        sent_dict[noun] += sentiment\n",
    "                    debug += 1\n",
    "    return sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of texts\n",
    "\n",
    "iphone_reviews = df[df['phone']=='IPhone X']\n",
    "iphonex = list(iphone_reviews.text)\n",
    "\n",
    "s9_reviews = df[df['phone']=='Samsung Galaxy S9']\n",
    "s9 = list(s9_reviews.text)\n",
    "\n",
    "# Join the list into one string of texts\n",
    "iphonex = ' '.join(iphonex)\n",
    "s9 = ' '.join(s9)\n",
    "\n",
    "iphonex_aspect_sentiment_scores = dict(feature_sentiment(iphonex))\n",
    "s9_ascpect_sentiment_scores = dict(feature_sentiment(s9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted dataframe of phone aspects and their sentiment scores\n",
    "iphonex = pd.DataFrame.from_dict(iphonex_aspect_sentiment_scores, orient='index')\n",
    "iphonex['score'] = iphonex[0]\n",
    "iphonex['aspects'] = iphonex.index\n",
    "iphonex = iphonex.sort_values(by=['score'], ascending=False)\n",
    "iphonex = iphonex.reset_index()\n",
    "iphonex = iphonex.drop([0, 'index'], axis=1)\n",
    "iphonex = iphonex.reindex(sorted(iphonex.columns), axis=1)\n",
    "\n",
    "s9 = pd.DataFrame.from_dict(s9_ascpect_sentiment_scores, orient='index')\n",
    "s9['score'] = s9[0]\n",
    "s9['aspects'] = s9.index\n",
    "s9 = s9.sort_values(by=['score'], ascending=False)\n",
    "s9 = s9.reset_index()\n",
    "s9 = s9.drop([0, 'index'], axis=1)\n",
    "s9 = s9.reindex(sorted(s9.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Separate dataframes from positive and negative aspects for each phones\n",
    "\n",
    "# IPhoneX\n",
    "iphonex_negative = iphonex[iphonex['score']<0]\n",
    "iphonex_negative['negative aspects'] = iphonex_negative['aspects']\n",
    "iphonex_negative = iphonex_negative.sort_values(by=['score'], ascending=True)\n",
    "iphonex_negative = iphonex_negative.reset_index()\n",
    "iphonex_negative = iphonex_negative.drop(['index', 'aspects'], axis=1)\n",
    "iphonex_negative = iphonex_negative.reindex(sorted(iphonex_negative), axis=1)\n",
    "\n",
    "iphonex_positive = iphonex[iphonex['score']>0]\n",
    "iphonex_positive['negative aspects'] = iphonex_positive['aspects']\n",
    "iphonex_positive = iphonex_positive.reset_index()\n",
    "iphonex_positive = iphonex_positive.drop(['index', 'aspects'], axis=1)\n",
    "iphonex_positive = iphonex_positive.reindex(sorted(iphonex_positive), axis=1)\n",
    "\n",
    "\n",
    "# s9\n",
    "s9_negative = s9[s9['score']<0]\n",
    "s9_negative['negative aspects'] = s9_negative['aspects']\n",
    "s9_negative = s9_negative.sort_values(by=['score'], ascending=True)\n",
    "s9_negative = s9_negative.reset_index()\n",
    "s9_negative = s9_negative.drop(['index', 'aspects'], axis=1)\n",
    "s9_negative = s9_negative.reindex(sorted(s9_negative), axis=1)\n",
    "\n",
    "s9_positive = s9[s9['score']>0]\n",
    "s9_positive['negative aspects'] = s9_positive['aspects']\n",
    "s9_positive = s9_positive.reset_index()\n",
    "s9_positive = s9_positive.drop(['index', 'aspects'], axis=1)\n",
    "s9_positive = s9_positive.reindex(sorted(s9_positive), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE\n",
    "    ## Peter Min's review_pipe function with only the argument for the aspect terms\n",
    "    ## Testing of review_pipe function to see sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAY NEED TO SCRAPE TEST DATA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
