{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies and modules\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iphonex_digtrends.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c784f2d8782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load each json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iphonex_digtrends.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0miphonex_digtrends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iphonex_gizmodo.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iphonex_digtrends.json'"
     ]
    }
   ],
   "source": [
    "# Load each json file\n",
    "with open('iphonex_digtrends.json') as f:\n",
    "    iphonex_digtrends = json.load(f)\n",
    "\n",
    "with open('iphonex_gizmodo.json') as f:\n",
    "    iphonex_gizmodo = json.load(f)\n",
    "\n",
    "with open('iphonex_techradar.json') as f:\n",
    "    iphonex_techradar = json.load(f)\n",
    "\n",
    "with open('S9_digtrends.json') as f:\n",
    "    S9_digtrends = json.load(f)\n",
    "\n",
    "with open('S9_gizmodo.json') as f:\n",
    "    S9_gizmodo = json.load(f)\n",
    "\n",
    "with open('S9_techradar.json') as f:\n",
    "    S9_techradar = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "\n",
    "    text = str(text).replace(\"\\n\", \"\")\n",
    "    text = str(text).replace(\"\\t\", \"\")\n",
    "    text = str(text).replace(\"\\\\n\", \"\")\n",
    "    text = str(text).replace(\"\\\\t\", \"\")\n",
    "    text = str(text).replace(\"\\\\\", \"\")\n",
    "    text = str(text).replace(\"xa0\", \" \")\n",
    "    text = str(text).replace(\"\\'\", \"\")\n",
    "    text = re.sub(\"<p>\", \"\", str(text))\n",
    "    text = re.sub(\"</p>\", \"\", str(text))\n",
    "    text = re.sub(\"</a>\", \"\", str(text))\n",
    "    text = re.sub('<[^>]+>', \"\", str(text))\n",
    "    text = str(text).replace(\"\\\\u2019\", \"\")\n",
    "    text = str(text).replace(\"\\\\u2013\", \"\")\n",
    "    text = str(text).replace(\"\\\\u2018\", \"\")\n",
    "    text = str(text).replace(\"\\\\u00a0\", \"\")\n",
    "    text = str(text).replace(\"\\\\u00a3\", \"\")\n",
    "    text = str(text).replace(\"\\u2014\", \"\")\n",
    "    text = str(text).replace(\"\\u201d\", \"\")\n",
    "    text = str(text).replace(\"\\u201c\", \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate each JSON file into a data frame\n",
    "\n",
    "iphonex_digtrends = pd.DataFrame.from_dict(iphonex_digtrends, orient='columns')\n",
    "iphonex_gizmodo = pd.DataFrame.from_dict(iphonex_gizmodo, orient='columns')\n",
    "iphonex_techradar = pd.DataFrame.from_dict(iphonex_techradar, orient='columns')\n",
    "S9_digtrends = pd.DataFrame.from_dict(S9_digtrends, orient='columns')\n",
    "S9_gizmodo = pd.DataFrame.from_dict(S9_gizmodo, orient='columns')\n",
    "S9_techradar = pd.DataFrame.from_dict(S9_techradar, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean text\n",
    "def clean_text(df):\n",
    "    # Convert lists to strings and remove brackets\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['author'] = df['author'].astype(str)\n",
    "\n",
    "    df['text'] = df['text'].map(lambda x: x.strip('[]'))\n",
    "    df['author'] = df['author'].map(lambda x: x.strip('[]'))\n",
    "\n",
    "    # Clean text\n",
    "    df['text'] = df['text'].apply(lambda x: text_cleaner(x))\n",
    "    df['title'] = df['title'].apply(lambda x: text_cleaner(x))\n",
    "    df['author'] = df['author'].apply(lambda x: text_cleaner(x))\n",
    "\n",
    "    \n",
    "# Put dataframes into a list to iterate through\n",
    "dataframes = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar, S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "\n",
    "# Clean each Data Frame\n",
    "for dataframe in dataframes:\n",
    "    clean_text(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label all the rows in the dataframe for the phone that the article is talking about\n",
    "\n",
    "iphones = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar]\n",
    "s9s = [S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "\n",
    "for dataframe in iphones:\n",
    "    dataframe['phone'] = 'IPhone X'\n",
    "    \n",
    "for dataframe in s9s:\n",
    "    dataframe['phone'] = 'Samsung Galaxy S9'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the dataframes into one dataframe\n",
    "all_frames = [iphonex_digtrends, iphonex_gizmodo, iphonex_techradar, S9_digtrends, S9_gizmodo, S9_techradar]\n",
    "df = pd.concat(all_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize text\n",
    "# df['text'] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "# df['title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "# # Remove Stopwords, or keep it, might be important for aspect based semantics\n",
    "# stop = stopwords.words('english')\n",
    "# df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# df['title'] = df['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# # Lowercase everything\n",
    "# df['text'] = df['text'].astype(str)\n",
    "# df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# df['title'] = df['title'].astype(str)\n",
    "# df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "\n",
    "# # remove all punctuations\n",
    "# df['text'] = df['text'].apply(lambda x: ''.join(c for c in x if c not in punctuation))\n",
    "# df['title'] = df['title'].apply(lambda x: ''.join(c for c in x if c not in punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "spacy.load('en')\n",
    "parser = English()\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens  = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wordnet to find meaning of words, synonyms and antonyms\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Function to lemmatize and more words to their root\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# Compile set of stopwords\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "# Prepare training set for LDA\n",
    "tokens = df['text'].apply(lambda x: prepare_text_for_lda(x))\n",
    "\n",
    "# Prepare Dataframe for later\n",
    "df['text'] = df['text'].apply(lambda x: prepare_text_for_lda(x))\n",
    "\n",
    "# Append tokenized text to list of tokenized data\n",
    "null = tokens.apply(lambda x: text_data.append(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean Title Column\n",
    "\n",
    "# Tokenize text\n",
    "df['title'] = df.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "# Remove Stopwords, or keep it, might be important for aspect based semantics\n",
    "stop = stopwords.words('english')\n",
    "df['title'] = df['title'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "# Lowercase everything\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['title'] = df['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Assemble tokenized text data into a dictionary\n",
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words corpus from text data\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Create TF-IDF vectors from our bag of words\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(corpus_tfidf, open('corpus_tfidf.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "\n",
    "# Extract Topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics = NUM_TOPICS, id2word = dictionary, passes=50)\n",
    "ldamodel.save('model3.gensim')\n",
    "\n",
    "# Print terms for topics\n",
    "topics = ldamodel.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 4\n",
    "\n",
    "# Extract Topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics = NUM_TOPICS, id2word = dictionary, passes=50)\n",
    "ldamodel.save('model4.gensim')\n",
    "\n",
    "# Print terms for topics\n",
    "topics = ldamodel.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "\n",
    "# Extract Topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics = NUM_TOPICS, id2word = dictionary, passes=50)\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "# Print terms for topics\n",
    "topics = ldamodel.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 7\n",
    "\n",
    "# Extract Topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics = NUM_TOPICS, id2word = dictionary, passes=50)\n",
    "ldamodel.save('model7.gensim')\n",
    "\n",
    "# Print terms for topics\n",
    "topics = ldamodel.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "\n",
    "# Extract Topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics = NUM_TOPICS, id2word = dictionary, passes=50)\n",
    "ldamodel.save('model10.gensim')\n",
    "\n",
    "# Print terms for topics\n",
    "topics = ldamodel.print_topics(num_words=7)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Visualizations of topic clusters using pyLDAvis\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('/Users/ayankarim/Documents/Thinkful/Bootcamp/Final Capstone Opinion Mining/opinion_mining/Notebook/corpus_tfidf.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "# Display 3 topics\n",
    "lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
